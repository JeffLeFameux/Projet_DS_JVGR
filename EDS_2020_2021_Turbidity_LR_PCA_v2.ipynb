{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Figures/LogoObsera.png\" width=\"75\" align=\"left\" style=\"vertical-align:middle;margin:0px 100px\" ><h1>   River load sensor calibration</h1>\n",
    "\n",
    "\n",
    "<h2>Find the regression for linking suspended load with high frequency turbidity</h2>\n",
    "\n",
    "-------\n",
    "\n",
    "- **Authors**: Antoine Lucas (lucas@ipgp.fr) - Amande Roque-Bernard (roquebernard@ipgp.fr) - Grégory Sainton (sainton@ipgp.fr)\n",
    "- **Version**: 2.0\n",
    "\n",
    "\n",
    "--------\n",
    "- **Objective**: We seek to calibrate a turbidity probe placed in a river against independent measurements of suspended solids concentration.\n",
    "\n",
    "- **The data**: The data come from ObsEra, an observatory located in Guadeloupe that gives us information on the erosion of this volcanic island in a few watersheds.\n",
    "\n",
    "- **You will learn**: how to prepare and clean the data. It's probably the most important part of the job. Then, you will play with your first models with the `scikit-learn` library. \n",
    "\n",
    "\n",
    "- **NB1**: If some errors or bugs are still in the notebook despite our efforts, please send us an email to tell us.\n",
    "\n",
    "- **NB2**: Solutions which will be proposed in the corrected notebooks are only a possible solution. \n",
    "\n",
    "- **NB3**: This notebook is based on Amande's thesis work. Further analysis can be done, please contact her. \n",
    "\n",
    "----\n",
    "## This notebook is made of two parts \n",
    "### How to open the data ? \n",
    "\n",
    "It's a very guided part to learn how to have a first look on your datasets. School data are often already clean but the real data are usually very messy. So we need to spend time to clean and to prepare the data before starting the real analysis.\n",
    "\n",
    "### Select, apply, train a model\n",
    "\n",
    "In this section, we will play with some models using `scikit-learn` library. \n",
    "1. Linear regression\n",
    "2. Principal Component Analysis (PCA)\n",
    "\n",
    "But of course, we encourage you to go further, to test some other models and to compare your results.\n",
    "\n",
    "Have fun !\n",
    "\n",
    "----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to open the data\n",
    "\n",
    "All the data are saved in the `data` directory of this lab.\n",
    "They are separeted in two subsets, one with chemical informations and another one with hydrological informations\n",
    "\n",
    "This part is done for you. No need to touch the following cell. Just notice that `filelist_chem` and `filelist_hydro` are the 2 array variables which contain the list of files we will use after."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DON'T MODIFY ANYTHING BELOW\n",
    "#%reset -f     \n",
    "# The previous line is used to reset all the variables at each runs\n",
    "\n",
    "import os, sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#Chemical data\n",
    "ObseraDir_chem = './data/CHEM/'\n",
    "filelist_chem = glob(ObseraDir_chem + 'C*.csv')\n",
    "\n",
    "#Hydrologic data\n",
    "ObseraDir_hydro = './data/HYDRO/'\n",
    "filelist_hydro = glob(ObseraDir_hydro + 'Data*.csv' )  # list of the raw data files        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observe and prepare the data\n",
    "The first step, is to look at your data. \n",
    "\n",
    "- For the first dataset, you'are guided at almost every steps. \n",
    "- For the second data set, you will have to do it on your own."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chemical data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Read the data\n",
    "\n",
    "1. Using the `read_csv` function from Pandas library, open the file (Use the parameter `na_value` to remove NaN) data.\n",
    "\n",
    "2. Create a \"datetime\" column by adding together the columns \"Date\" and \"Hour\"   ```2020-01-06 10:00:00```\n",
    "3. Convert this coloumn to a datatime format.\n",
    "4. Attribute the column `datetime` as the index of your pandas structure.\n",
    "\n",
    "**hints** \n",
    "- Beware of the separator which is a semicolon (;)\n",
    "- `filelist_chem` is an array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WRITE YOUR CODE HERE\n",
    "data_chem = None                           # open the file "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Inspect the data\n",
    "Using `data_chem.head()`, `data_chem.info()`,`data_chem.describe()`, explore your data set\n",
    "\n",
    "\n",
    "- numbers of rows, of columns\n",
    "- checks the range of each fields\n",
    "- What are the different fields, their type (especially, non numeric values).\n",
    "- Do we have NaN, blank values ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WRITE YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Extract datetime and suspended Load data only\n",
    "\n",
    "In fact, for this exercice, we only need to keep the fields `datetime` and `Suspended Load` and `Level`\n",
    "\n",
    "1. Create a new pandas DataFrame only containing\n",
    "2. Remove negative value and NaN value\n",
    "3. Plot the `Suspended Load` in function of the `datetime` (You can use either Matplotlib or Seaborn (or whatever))\n",
    "\n",
    "**hints**\n",
    "- One can remove negative value by creating a mask\n",
    "- For the NaN removal, use the Pandas `.dropna()` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WRITE YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that your plot looks like \n",
    "<img src=\"./Figures/Plot_SuspendedLoadOvertime.png\" width = \"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hydrological data\n",
    "\n",
    "In this second part, you will have to deal with the files in the variable `filelist_hydro`\n",
    "\n",
    "1. Open all the files and create a unique DataFrace called `data_hydro`.\n",
    "2. Inspect your data.\n",
    "3. Plot the `turbidity` in function of the `datetime`\n",
    "\n",
    "\n",
    "**hints**\n",
    "- Create a first DataFrame with the first file and then concatenate some other DataFrame made with the other files\n",
    "- Think about using `pd.concat` to concatenate the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WRITE YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synchronize the data\n",
    "The goal here is to get the same time vector on the both structure. But the sampling is not the same for the both set of data.\n",
    "\n",
    "To ease your life, the next part is done for you. Keep the method somewhere in case you need to use it someday.\n",
    "\n",
    "- We first check the common interval\n",
    "- Upsample the date to get the same sampling as data_chem\n",
    "- Match the data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synchronisation : the tricks \n",
    "\n",
    "You just have to execute the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = data_chem['datetime'] >= data_hydro.datetime[0] # premiere date de l'hydro\n",
    "data_chem = data_chem.loc[mask]\n",
    "\n",
    "del mask\n",
    "\n",
    "mask    = data_chem['Suspended Load'] >= 0.\n",
    "data_ch = data_chem.loc[mask]\n",
    "\n",
    "turbidity   = data_hydro[[\"turbidity\"]]\n",
    "water_level = data_hydro[[\"level\"]]\n",
    "\n",
    "upsampled   = turbidity.resample('1T')\n",
    "turbidity_by_minute = upsampled.interpolate(method='linear')\n",
    "\n",
    "upsampled   = water_level.resample('1T')\n",
    "water_level_by_minute = upsampled.interpolate(method='linear')\n",
    "\n",
    "del upsampled\n",
    "\n",
    "# How to use pandas to find consecutive same data in time series\n",
    "# https://stackoverflow.com/questions/26911851/how-to-use-pandas-to-find-consecutive-same-data-in-time-series\n",
    "\n",
    "turbidity['value_grp'] = (np.isnan(turbidity.turbidity)).astype('int')\n",
    "turbidity['value_grp'] = (turbidity['value_grp'].diff(1) != 0).astype('int').cumsum()\n",
    "turbidity['Date'] = turbidity.index\n",
    "\n",
    "\n",
    "water_level['value_grp'] = (np.isnan(water_level.level)).astype('int')\n",
    "water_level['value_grp'] = (water_level['value_grp'].diff(1) != 0).astype('int').cumsum()\n",
    "water_level['Date']      = water_level.index\n",
    "\n",
    "\n",
    "\n",
    "# decoupage par paquets de donnees de turbidity\n",
    "check = pd.DataFrame({'BeginDate' : turbidity.groupby('value_grp').Date.first(), \n",
    "              'EndDate' : turbidity.groupby('value_grp').Date.last(),\n",
    "              'Consecutive' : turbidity.groupby('value_grp').size(),\n",
    "              'sum' : turbidity.groupby('value_grp').turbidity.sum().astype('float')}).reset_index(drop=True)   \n",
    "\n",
    "    \n",
    "for i in range(len(check)):\n",
    "    if check['sum'][i] > 0. : # le morceau contient des donnees car la somme des valeurs est positive\n",
    "        continue\n",
    "    else:\n",
    "        if check['Consecutive'][i] > 3. :  # le chunk contient trop de nan pour une interpolation correct \n",
    "                                           # donc valeurs de linterpolation changé en NAN\n",
    "            turbidity_by_minute[check['BeginDate'][i]:check['EndDate'][i]] = np.NaN\n",
    "            water_level_by_minute[check['BeginDate'][i]:check['EndDate'][i]] = np.NaN\n",
    "\n",
    "       \n",
    "common = turbidity_by_minute.index.intersection(data_ch['Suspended Load'].index)\n",
    "suspended_load = data_ch['Suspended Load'].loc[common]\n",
    "\n",
    "turbidity_by_minute            = turbidity_by_minute.loc[common]\n",
    "matching_turbidity_by_minute   = turbidity_by_minute.loc[common]\n",
    "matching_water_level_by_minute = water_level_by_minute.loc[common]\n",
    "\n",
    "turbidity_by_minute   = matching_turbidity_by_minute        \n",
    "water_level_by_minute = matching_water_level_by_minute    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,6))\n",
    "\n",
    "ax1 = plt.subplot(311)\n",
    "plt.plot(suspended_load[:],'.')\n",
    "plt.grid()\n",
    "plt.ylabel('$C_{S}$ [mg/L]')\n",
    "\n",
    "ax2 = plt.subplot(312, sharex=ax1)\n",
    "plt.plot(matching_turbidity_by_minute,'.')\n",
    "plt.ylabel('T [NTU]')\n",
    "plt.grid()\n",
    "fig.tight_layout()\n",
    "\n",
    "ax3 = plt.subplot(313, sharex=ax1)\n",
    "plt.plot(matching_water_level_by_minute,'.')\n",
    "plt.ylabel('h [cm]')\n",
    "plt.grid()\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SS   = np.array(suspended_load)\n",
    "TUR  = np.array(matching_turbidity_by_minute)\n",
    "H    = np.array(matching_water_level_by_minute)\n",
    "\n",
    "SS  = np.squeeze(SS)\n",
    "TUR = np.squeeze(TUR)\n",
    "H   = np.squeeze(H)\n",
    "\n",
    "mask = ~np.isnan(TUR)\n",
    "\n",
    "SS   = SS[mask]\n",
    "TUR  = TUR[mask]\n",
    "H    = H[mask]\n",
    "\n",
    "TUR1,SS = zip(*sorted(zip(TUR,SS)))\n",
    "TUR,H = zip(*sorted(zip(TUR,H)))\n",
    "\n",
    "del TUR1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame({'turbidity':TUR, 'level':H, 'Suspended Load': SS})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final DataFrame you will have to deal with is `data`\n",
    "\n",
    "**Have a quick view on your data**\n",
    "\n",
    "Using `scatter_matrix` from pandas, one can have a look on all the numerical variables of the dataset. \n",
    "- Diagonals show the distribution of the variable\n",
    "- Other combinations show the scatter plot of a given variable vs another one.\n",
    "\n",
    "\n",
    "**Make two plots (horizontal subplots)**:\n",
    "1. Suspended Load (Units = $C_s(mg/L)$) in function of the Turbidity (Units = $NTU$)\n",
    "2. Suspended Load in function of the Level (Units = cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Have a quick view on your data -> WRITE YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make two plots -> WRITE YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that your plot looks like\n",
    "\n",
    "\n",
    "<img src=\"./Figures/Plot Cs_h.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select, apply, train a model\n",
    "\n",
    "It's now time to fit the model. We are trying several models...\n",
    "\n",
    "\n",
    "But before, let us give you a **Quick remind about the Scikit-Learn Design** \n",
    "It's a quote from Aurelien Geron, _Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "** Quick remind about the Scikit-Learn Design** (from A. Geron, _Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow_)\n",
    "\n",
    "Scikit-Learn’s API is remarkably well designed. The main principles are \n",
    "\n",
    "**Consistency**. All objects share a consistent and simple interface:\n",
    "\n",
    "- **Estimators**. Any object that can estimate some parameters based on a dataset is called an estimator (e.g., an imputer is an estimator). The estimation itself is performed by the fit() method, and it takes only a dataset as a parameter (or two for supervised learning algorithms; the second dataset contains the labels). Any other parameter needed to guide the estimation process is con‐ sidered a hyperparameter (such as an imputer’s strategy), and it must be set as an instance variable (generally via a constructor parameter).\n",
    "\n",
    "- **Transformers**. Some estimators (such as an imputer) can also transform a dataset; these are called transformers. Once again, the API is quite simple: the transformation is performed by the transform() method with the dataset to transform as a parameter. It returns the transformed dataset. This transforma‐ tion generally relies on the learned parameters, as is the case for an imputer. All transformers also have a convenience method called fit_transform() that is equivalent to calling fit() and then transform() (but sometimes fit_transform() is optimized and runs much faster).\n",
    "\n",
    "- **Predictors**. Finally, some estimators are capable of making predictions given a dataset; they are called predictors. For example, the LinearRegression model in the previous chapter was a predictor: it predicted life satisfaction given a country’s GDP per capita. A predictor has a predict() method that takes a dataset of new instances and returns a dataset of corresponding predictions. It also has a score() method that measures the quality of the predictions given a test set (and the corresponding labels in the case of supervised learning algorithms).\n",
    "\n",
    "- **Inspection**. All the estimator’s hyperparameters are accessible directly via public instance variables (e.g., imputer.strategy), and all the estimator’s learned parameters are also accessible via public instance variables with an underscore suffix (e.g., imputer.statistics_).\n",
    "\n",
    "- **Nonproliferation of classes**. Datasets are represented as NumPy arrays or SciPy sparse matrices, instead of homemade classes. Hyperparameters are just regular Python strings or numbers.\n",
    "\n",
    "- **Composition**. Existing building blocks are reused as much as possible. For example, it is easy to create a Pipeline estimator from an arbitrary sequence of transformers followed by a final estimator, as we will see.\n",
    "\n",
    "- **Sensible defaults**. Scikit-Learn provides reasonable default values for most parameters, making it easy to create a baseline working system quickly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear regression\n",
    "\n",
    "No need to use **scikit-learn** to do linear regression but let's learn to do with this library anyway as an introduction to it.\n",
    "\n",
    "To import the Linear Regression methods : \n",
    "`from sklearn.linear_model import LinearRegression`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Questions\n",
    "\n",
    "1. Apply a classic linear regression on the data : $SS = f(TUR)$ \n",
    "\n",
    "    - Find the coefficients of the fit\n",
    "    - Give the score of the result\n",
    "    \n",
    "\n",
    "2. How good is your model ? \n",
    "\n",
    "    - Apply your model to predict values using your input dataset: `myprediction = model.predict(dataset)`\n",
    "    - Measure the **MAE** (Mean Absolute Error) to evaluate the quality of your model: \n",
    "`from sklearn.metrics import mean_absolute_error`\n",
    "\n",
    "`mymae = mean_absolute_error(myprediction-labeldata)`\n",
    "\n",
    "In our case, `labeldata` is the Suspended Load vector\n",
    "\n",
    "\n",
    "3. Check your result with the log or your data to see if the results are improved.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WRITE YOUR CODE HERE\n",
    "\n",
    "#Remane the vectors\n",
    "\n",
    "TUR= data[\"turbidity\"].to_numpy().reshape(len(data[\"turbidity\"]),1)\n",
    "SS = data[\"Suspended Load\"].to_numpy().reshape(len(data[\"Suspended Load\"]),1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, try to fit your data in the other direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WRITE YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Robust linear regression\n",
    "\n",
    "As you can see, the data have some outiers. Is is possible to improve the result of the fit ? \n",
    "\n",
    "Just as an example, apply a RANSAC regression on the data.\n",
    "- Coefficient ? \n",
    "- Plot of the fit\n",
    "- Score of the result\n",
    "- Comparison with the classical LR -> Comments ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WRITE YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Plot, comparison, conclusion\n",
    "\n",
    "1. Plot the data and your different fits on the same plot\n",
    "2. Any comments about the linear regression on these data ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction to PCA\n",
    "\n",
    "As you've seen during the lectures, *Principal Component Analysis* (PCA) is by far the most popular dimensionality reduction algorithm. First it identifies the hyperplane that lies closest to the data, and then it projects the data onto it.\n",
    "\n",
    "But what lower dimensional hyperplane to choose ? We have to choose one axis which preserve the maximum variance. From math point of view, it is the axis which minimizes the mean square distribution between the original data and its projection on the axis. \n",
    "\n",
    "\n",
    "In this part, we are going to apply the PCA to modelise the relation between the turbidity and the suspended load. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quick remind on what the PCA is. \n",
    "\n",
    "By definition, the PCA is the way to project the data in a lower dimension base. The base is choosed in order to maximize the variance of the projected data.\n",
    "\n",
    "For example: \n",
    "\n",
    "- Let's say that we have data in 2 dimensions ($D=2$) and we want to find project into a single dimension space ($M=1$ with the vector $u_1$). \n",
    "\n",
    "The variance of the projected data in $M=1$ is \n",
    "\n",
    "$\\begin{equation}\n",
    "V = \\frac{1}{N}\\sum\\limits_{n=1}^{N} (u_1^Tx_n -u_1^T\\bar{x})^2 \\tag{1}\n",
    "\\end{equation}$ \n",
    "\n",
    "where $u_1^Tx_n$ is the projection of the point $x_n$ in the $u_1$ space. \n",
    "\n",
    "$\\begin{align*}\n",
    "V &= \\frac{1}{N}\\sum\\limits_{n=1}^{N} (u_1^T(x_n -\\bar{x}))^2 \\\\\n",
    "          &=\\frac{1}{N}\\sum\\limits_{n=1}^{N} u_1^T.(x_n -\\bar{x}).(x_n -\\bar{x})^T.u_1 \\\\\n",
    "          &=u_1^T.\\big\\lbrace\\frac{1}{N}\\sum\\limits_{n=1}^{N} (x_n -\\bar{x}).(x_n -\\bar{x})^T)\\big\\rbrace.u_1 \\\\ \\\\\n",
    "          &= u_1^T.S.u_1\n",
    "\\end{align*} \\tag{2}$ \n",
    "\n",
    "where $S$ is the covariance matrix and $\\bar{x}$ is the average of the data $x_n$. \n",
    "\n",
    "Now, we have to maximize the variance. We constraint the norm of $u_1$ to be equal to $1$ and then, we maximize.\n",
    "\n",
    "It can be easely expressed with the [Lagrangian multipliers](https://en.wikipedia.org/wiki/Lagrange_multiplier). \n",
    "\n",
    "$\\begin{equation}\n",
    "\\mathcal{L} = u_1^T.S.u_1 + \\lambda_1(1-u_1^Tu_1)\n",
    "\\tag{3}\n",
    "\\end{equation} $ \n",
    "\n",
    "Then we estimate the gradient in regards to $u_1$ which is equal to $0$.\n",
    "\n",
    "Finally after calculations, we obtain:\n",
    "\n",
    "$\\begin{equation}\n",
    "Su_1 = \\lambda_1u_1\n",
    "\\tag{4}\n",
    "\\end{equation}$ \n",
    "\n",
    "It means that $u_1$ is the eigenvector of $S$ and $\\lambda_1$ the eigenvalue. \n",
    "\n",
    "So, if $u_1$ is an eigenvector of $S$, the variance is equal to:\n",
    "\n",
    "$\\begin{equation}\n",
    "V= u_1^T.S.u_1 = u_1^T.(\\lambda_1.u_1) = \\lambda_1\n",
    "\\tag{5}\n",
    "\\end{equation}$ \n",
    "\n",
    "So, to maximize the variance, we need to take de eigenvector $u_1$ corresponding to the highest eigenvalue $\\lambda_1$.\n",
    "\n",
    "- In case $M>1$, we are dealing the problem iterativaly. Each time, we are looking for another projection which maximize the variance and which is orthogonal to the previous ones. \n",
    "\n",
    "The projected data $y(x)$ on the $M$ dimensionnal base is finally given by the following expression:\n",
    "\n",
    "$\\begin{equation}\n",
    "y(x) = (U_{:,1:M})^T.x\n",
    "\\tag{6}\n",
    "\\end{equation}$ \n",
    "where $U$ is the matrix of the eigenvectors of the covariance matrix $S$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Back to the problem... \n",
    "\n",
    "Let's reduce the notation by using the two following variables for the turbidity and the suspended load values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TUR = data[\"turbidity\"]\n",
    "SS  = data[\"Suspended Load\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Questions\n",
    "1. Using `numpy`, estimate the covariance matrix of the log of theses two vectors\n",
    "2. Find the Eigen values and the Eigen vectors of the covariance matrix.\n",
    "3. Calculate the coefficients of the linear equation using the Eigen values.\n",
    "\n",
    "**hint**\n",
    "`np.cov()` and `np.log` and `np.linalg.eig` could be useful here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WRITE YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact the relation between the turbidity and the suspended load can be modelised as a _power low_\n",
    "\n",
    "\n",
    "SL_acp = np.exp(b_acp)*(TUR_x**a_acp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Questions\n",
    "1. Create a turbidity vector `TUR_x` from $[0,1000]$\n",
    "2. Create a vector `SL_acp` using the coefficients of the PCA over the values of `TUR_x`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WRITE YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that your plot looks like"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Figures/Plot_PCA_fit.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA with Scikit-learn (bonus)\n",
    "\n",
    "This section is just to show you that everything you have calculated using the linear algebra is already implemented in `scikit-learn`\n",
    "\n",
    "The next cells are just here to show you how to do it. Of course, don't hesitate to refer to the documentation about PCA = \n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
